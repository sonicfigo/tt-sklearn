#### 几个概念

----
TF: 
Term Frequency, 用于衡量一个词在一个文件中的出现频率。

因为每个文档的长度的差别可以很大，因而一个词在某个文档中出现的次数可能远远大于另一个文档，
所以词频通常就是一个词出现的次数除以文档的总长度，相当于是做了一次归一化。

TF(t) = (词t在文档中出现的总次数) / (文档的词总数).

----
IDF: 逆向文件频率，用于衡量一个词的重要性。

计算词频TF的时候，所有的词语都被当做一样重要的，但是某些词，比如”is”, “of”, “that”很可能出现很多很多次，
但是可能根本并不重要，因此我们需要减轻在多个文档中都频繁出现的词的权重。 

ID(t) = log_e(总文档数/词t出现的文档数)


#### 接触到的 sklearn.feature_extraction.text 这个包下的3个常用类

tokenizer，提取词组
1. CountVectorizer
2. TfidfVectorizer，是1的派生类

特征加权
而文本特征权重的计算方法有许多，scikit-learn貌似也只提供了TF-TDF这一种。
3. TfidfTransformer 
